{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a4e176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering, DBSCAN, KMeans\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "# Plotly removido por compatibilidad con NumPy\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def verify_and_sync_data(distance_matrix, clustering_results, series_matrix):\n",
    "    \"\"\"\n",
    "    Verifica y sincroniza los datos para asegurar compatibilidad\n",
    "    \"\"\"\n",
    "    print(\"=== VERIFICACIÓN Y SINCRONIZACIÓN DE DATOS ===\")\n",
    "    \n",
    "    # Verificar tipos de índices\n",
    "    print(f\"Tipos de datos:\")\n",
    "    print(f\"  distance_matrix.index: {type(distance_matrix.index[0])}\")\n",
    "    print(f\"  series_matrix.index: {type(series_matrix.index[0])}\")  \n",
    "    print(f\"  clustering_results.product_id: {type(clustering_results['product_id'].iloc[0])}\")\n",
    "    \n",
    "    # Convertir todos a string para comparación\n",
    "    distance_products = set(str(x) for x in distance_matrix.index)\n",
    "    series_products = set(str(x) for x in series_matrix.index)\n",
    "    cluster_products = set(str(x) for x in clustering_results['product_id'])\n",
    "    \n",
    "    print(f\"\\nTamaños de conjuntos:\")\n",
    "    print(f\"  distance_matrix: {len(distance_products)} productos\")\n",
    "    print(f\"  series_matrix: {len(series_products)} productos\")\n",
    "    print(f\"  clustering_results: {len(cluster_products)} productos\")\n",
    "    \n",
    "    # Encontrar intersecciones\n",
    "    common_all = distance_products & series_products & cluster_products\n",
    "    common_dist_cluster = distance_products & cluster_products\n",
    "    common_series_cluster = series_products & cluster_products\n",
    "    \n",
    "    print(f\"\\nIntersecciones:\")\n",
    "    print(f\"  Común en los 3: {len(common_all)} productos\")\n",
    "    print(f\"  Común distance + cluster: {len(common_dist_cluster)} productos\")\n",
    "    print(f\"  Común series + cluster: {len(common_series_cluster)} productos\")\n",
    "    \n",
    "    # Encontrar productos faltantes\n",
    "    missing_in_distance = cluster_products - distance_products\n",
    "    missing_in_series = cluster_products - series_products\n",
    "    \n",
    "    if missing_in_distance:\n",
    "        print(f\"\\nProductos en clustering pero NO en distance_matrix: {len(missing_in_distance)}\")\n",
    "        print(f\"  Ejemplos: {list(missing_in_distance)[:5]}\")\n",
    "        \n",
    "    if missing_in_series:\n",
    "        print(f\"\\nProductos en clustering pero NO en series_matrix: {len(missing_in_series)}\")\n",
    "        print(f\"  Ejemplos: {list(missing_in_series)[:5]}\")\n",
    "    \n",
    "    # Sincronizar clustering_results para usar solo productos disponibles\n",
    "    if missing_in_distance or missing_in_series:\n",
    "        print(f\"\\n=== SINCRONIZANDO DATOS ===\")\n",
    "        available_products = common_all\n",
    "        \n",
    "        # Filtrar clustering_results\n",
    "        clustering_results_sync = clustering_results[\n",
    "            clustering_results['product_id'].astype(str).isin(available_products)\n",
    "        ].copy()\n",
    "        \n",
    "        print(f\"Clustering sincronizado: {len(clustering_results_sync)} productos (era {len(clustering_results)})\")\n",
    "        \n",
    "        # Verificar que todos los clusters siguen teniendo productos\n",
    "        cluster_counts = clustering_results_sync['cluster'].value_counts()\n",
    "        print(f\"Distribución de clusters después de sincronización:\")\n",
    "        for cluster, count in cluster_counts.items():\n",
    "            print(f\"  Cluster {cluster}: {count} productos\")\n",
    "        \n",
    "        return clustering_results_sync\n",
    "    else:\n",
    "        print(\"\\n✓ Todos los datos están sincronizados\")\n",
    "        return clustering_results\n",
    "\n",
    "def load_dtw_results(distance_matrix_path='dtw_matrix_full.csv'):\n",
    "    \"\"\"\n",
    "    Carga la matriz de distancias DTW previamente calculada\n",
    "    \"\"\"\n",
    "    print(\"Cargando matriz de distancias DTW...\")\n",
    "    distance_df = pd.read_csv(distance_matrix_path, index_col=0)\n",
    "    \n",
    "    # Convertir índices a enteros si es posible\n",
    "    try:\n",
    "        distance_df.index = distance_df.index.astype(int)\n",
    "        distance_df.columns = distance_df.columns.astype(int)\n",
    "        print(\"Índices convertidos a enteros\")\n",
    "    except:\n",
    "        print(\"Manteniendo índices como están\")\n",
    "    \n",
    "    print(f\"Matriz cargada: {distance_df.shape}\")\n",
    "    print(f\"Productos: {len(distance_df)}\")\n",
    "    print(f\"Primeros índices: {list(distance_df.index[:5])}\")\n",
    "    \n",
    "    # Verificar que es simétrica\n",
    "    is_symmetric = np.allclose(distance_df.values, distance_df.values.T, equal_nan=True)\n",
    "    print(f\"Matriz simétrica: {is_symmetric}\")\n",
    "    \n",
    "    return distance_df\n",
    "\n",
    "def find_optimal_clusters(distance_matrix, max_clusters=20, methods=['hierarchical', 'kmeans']):\n",
    "    \"\"\"\n",
    "    Encuentra el número óptimo de clusters usando diferentes métodos\n",
    "    \"\"\"\n",
    "    print(\"=== BÚSQUEDA DE NÚMERO ÓPTIMO DE CLUSTERS ===\")\n",
    "    \n",
    "    # Convertir a matriz de distancias para sklearn\n",
    "    distance_array = distance_matrix.values\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for method in methods:\n",
    "        print(f\"\\nAnalizando método: {method}\")\n",
    "        \n",
    "        silhouette_scores = []\n",
    "        calinski_scores = []\n",
    "        davies_bouldin_scores = []\n",
    "        inertias = []\n",
    "        \n",
    "        cluster_range = range(2, min(max_clusters + 1, len(distance_matrix) - 1))\n",
    "        \n",
    "        for n_clusters in cluster_range:\n",
    "            try:\n",
    "                if method == 'hierarchical':\n",
    "                    clustering = AgglomerativeClustering(\n",
    "                        n_clusters=n_clusters,\n",
    "                        metric='precomputed',\n",
    "                        linkage='average'\n",
    "                    )\n",
    "                    labels = clustering.fit_predict(distance_array)\n",
    "                    \n",
    "                elif method == 'kmeans':\n",
    "                    # Para K-means necesitamos embeddings\n",
    "                    mds = MDS(n_components=10, dissimilarity='precomputed', random_state=42)\n",
    "                    embeddings = mds.fit_transform(distance_array)\n",
    "                    \n",
    "                    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "                    labels = kmeans.fit_predict(embeddings)\n",
    "                    inertias.append(kmeans.inertia_)\n",
    "                \n",
    "                # Calcular métricas\n",
    "                if len(np.unique(labels)) > 1:\n",
    "                    sil_score = silhouette_score(distance_array, labels, metric='precomputed')\n",
    "                    silhouette_scores.append(sil_score)\n",
    "                    \n",
    "                    # Para Calinski-Harabasz y Davies-Bouldin necesitamos embeddings\n",
    "                    if method == 'hierarchical':\n",
    "                        mds_temp = MDS(n_components=10, dissimilarity='precomputed', random_state=42)\n",
    "                        embeddings_temp = mds_temp.fit_transform(distance_array)\n",
    "                    else:\n",
    "                        embeddings_temp = embeddings\n",
    "                    \n",
    "                    cal_score = calinski_harabasz_score(embeddings_temp, labels)\n",
    "                    db_score = davies_bouldin_score(embeddings_temp, labels)\n",
    "                    \n",
    "                    calinski_scores.append(cal_score)\n",
    "                    davies_bouldin_scores.append(db_score)\n",
    "                else:\n",
    "                    silhouette_scores.append(-1)\n",
    "                    calinski_scores.append(0)\n",
    "                    davies_bouldin_scores.append(float('inf'))\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error con {n_clusters} clusters: {e}\")\n",
    "                silhouette_scores.append(-1)\n",
    "                calinski_scores.append(0)\n",
    "                davies_bouldin_scores.append(float('inf'))\n",
    "                if method == 'kmeans':\n",
    "                    inertias.append(float('inf'))\n",
    "        \n",
    "        results[method] = {\n",
    "            'n_clusters': list(cluster_range),\n",
    "            'silhouette': silhouette_scores,\n",
    "            'calinski_harabasz': calinski_scores,\n",
    "            'davies_bouldin': davies_bouldin_scores\n",
    "        }\n",
    "        \n",
    "        if method == 'kmeans':\n",
    "            results[method]['inertia'] = inertias\n",
    "        \n",
    "        # Encontrar óptimos\n",
    "        if silhouette_scores:\n",
    "            best_sil = cluster_range[np.argmax(silhouette_scores)]\n",
    "            best_cal = cluster_range[np.argmax(calinski_scores)]\n",
    "            best_db = cluster_range[np.argmin(davies_bouldin_scores)]\n",
    "            \n",
    "            print(f\"  Mejor Silhouette: {best_sil} clusters (score: {max(silhouette_scores):.3f})\")\n",
    "            print(f\"  Mejor Calinski-Harabasz: {best_cal} clusters (score: {max(calinski_scores):.1f})\")\n",
    "            print(f\"  Mejor Davies-Bouldin: {best_db} clusters (score: {min(davies_bouldin_scores):.3f})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_cluster_metrics(results):\n",
    "    \"\"\"\n",
    "    Visualiza las métricas de clustering\n",
    "    \"\"\"\n",
    "    n_methods = len(results)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'purple']\n",
    "    \n",
    "    for i, (method, data) in enumerate(results.items()):\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Silhouette Score\n",
    "        axes[0].plot(data['n_clusters'], data['silhouette'], \n",
    "                    marker='o', label=f'{method}', color=color)\n",
    "        \n",
    "        # Calinski-Harabasz Score\n",
    "        axes[1].plot(data['n_clusters'], data['calinski_harabasz'], \n",
    "                    marker='s', label=f'{method}', color=color)\n",
    "        \n",
    "        # Davies-Bouldin Score\n",
    "        axes[2].plot(data['n_clusters'], data['davies_bouldin'], \n",
    "                    marker='^', label=f'{method}', color=color)\n",
    "        \n",
    "        # Inertia (solo para K-means)\n",
    "        if 'inertia' in data:\n",
    "            axes[3].plot(data['n_clusters'], data['inertia'], \n",
    "                        marker='d', label=f'{method} - Inertia', color=color)\n",
    "    \n",
    "    # Configurar gráficos\n",
    "    axes[0].set_title('Silhouette Score (mayor es mejor)')\n",
    "    axes[0].set_xlabel('Número de Clusters')\n",
    "    axes[0].set_ylabel('Silhouette Score')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[1].set_title('Calinski-Harabasz Score (mayor es mejor)')\n",
    "    axes[1].set_xlabel('Número de Clusters')\n",
    "    axes[1].set_ylabel('Calinski-Harabasz Score')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[2].set_title('Davies-Bouldin Score (menor es mejor)')\n",
    "    axes[2].set_xlabel('Número de Clusters')\n",
    "    axes[2].set_ylabel('Davies-Bouldin Score')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    axes[3].set_title('Inertia - Método del Codo')\n",
    "    axes[3].set_xlabel('Número de Clusters')\n",
    "    axes[3].set_ylabel('Inertia')\n",
    "    axes[3].legend()\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def perform_hierarchical_clustering(distance_matrix, n_clusters=None):\n",
    "    \"\"\"\n",
    "    Realiza clustering jerárquico\n",
    "    \"\"\"\n",
    "    print(\"=== CLUSTERING JERÁRQUICO ===\")\n",
    "    \n",
    "    # Convertir a formato condensado para scipy\n",
    "    distance_array = distance_matrix.values\n",
    "    \n",
    "    # Linkage matrix\n",
    "    linkage_matrix = linkage(squareform(distance_array), method='average')\n",
    "    \n",
    "    # Si no se especifica n_clusters, usar dendrograma para decidir\n",
    "    if n_clusters is None:\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        dendrogram(linkage_matrix, labels=distance_matrix.index.tolist(), \n",
    "                  leaf_rotation=90, leaf_font_size=8)\n",
    "        plt.title('Dendrograma - Clustering Jerárquico')\n",
    "        plt.xlabel('Productos')\n",
    "        plt.ylabel('Distancia DTW')\n",
    "        plt.show()\n",
    "        \n",
    "        # Sugerir número de clusters basado en la estructura\n",
    "        n_clusters = 8  # Valor por defecto\n",
    "        print(f\"Usando {n_clusters} clusters por defecto\")\n",
    "    \n",
    "    # Obtener clusters\n",
    "    cluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Crear DataFrame con resultados\n",
    "    clustering_results = pd.DataFrame({\n",
    "        'product_id': distance_matrix.index,\n",
    "        'cluster': cluster_labels\n",
    "    })\n",
    "    \n",
    "    # Estadísticas\n",
    "    cluster_counts = clustering_results['cluster'].value_counts().sort_index()\n",
    "    print(f\"\\nDistribución de clusters:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"  Cluster {cluster}: {count} productos\")\n",
    "    \n",
    "    return clustering_results, linkage_matrix\n",
    "\n",
    "def perform_dbscan_clustering(distance_matrix, eps=None, min_samples=5):\n",
    "    \"\"\"\n",
    "    Realiza clustering DBSCAN\n",
    "    \"\"\"\n",
    "    print(\"=== CLUSTERING DBSCAN ===\")\n",
    "    \n",
    "    distance_array = distance_matrix.values\n",
    "    \n",
    "    # Si no se especifica eps, usar análisis de distancias\n",
    "    if eps is None:\n",
    "        # Calcular distancias promedio al k-ésimo vecino más cercano\n",
    "        k = min_samples\n",
    "        distances = []\n",
    "        for i in range(len(distance_array)):\n",
    "            row_distances = np.sort(distance_array[i])\n",
    "            distances.append(row_distances[k])  # k-ésimo vecino más cercano\n",
    "        \n",
    "        distances = np.sort(distances)\n",
    "        \n",
    "        # Plotear para encontrar el \"codo\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(distances)\n",
    "        plt.xlabel('Puntos ordenados por distancia')\n",
    "        plt.ylabel(f'Distancia al {k}-ésimo vecino más cercano')\n",
    "        plt.title('Análisis de Eps para DBSCAN')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        # Sugerir eps basado en el percentil 95\n",
    "        eps = np.percentile(distances, 95)\n",
    "        print(f\"Eps sugerido: {eps:.2f}\")\n",
    "    \n",
    "    # Aplicar DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric='precomputed')\n",
    "    cluster_labels = dbscan.fit_predict(distance_array)\n",
    "    \n",
    "    # Crear DataFrame con resultados\n",
    "    clustering_results = pd.DataFrame({\n",
    "        'product_id': distance_matrix.index,\n",
    "        'cluster': cluster_labels\n",
    "    })\n",
    "    \n",
    "    # Estadísticas\n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    \n",
    "    print(f\"\\nResultados DBSCAN:\")\n",
    "    print(f\"  Número de clusters: {n_clusters}\")\n",
    "    print(f\"  Puntos de ruido: {n_noise}\")\n",
    "    print(f\"  Eps usado: {eps:.2f}\")\n",
    "    print(f\"  Min samples: {min_samples}\")\n",
    "    \n",
    "    cluster_counts = clustering_results['cluster'].value_counts().sort_index()\n",
    "    print(f\"\\nDistribución de clusters:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        cluster_name = \"Ruido\" if cluster == -1 else f\"Cluster {cluster}\"\n",
    "        print(f\"  {cluster_name}: {count} productos\")\n",
    "    \n",
    "    return clustering_results\n",
    "\n",
    "def visualize_clusters_2d(distance_matrix, clustering_results, method='MDS'):\n",
    "    \"\"\"\n",
    "    Visualiza clusters en 2D usando reducción de dimensionalidad\n",
    "    \"\"\"\n",
    "    print(f\"=== VISUALIZACIÓN 2D - {method} ===\")\n",
    "    \n",
    "    distance_array = distance_matrix.values\n",
    "    \n",
    "    # Reducción de dimensionalidad\n",
    "    if method == 'MDS':\n",
    "        reducer = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "        coords_2d = reducer.fit_transform(distance_array)\n",
    "    elif method == 'TSNE':\n",
    "        # t-SNE necesita embeddings, no distancias directas\n",
    "        mds = MDS(n_components=10, dissimilarity='precomputed', random_state=42)\n",
    "        embeddings = mds.fit_transform(distance_array)\n",
    "        reducer = TSNE(n_components=2, random_state=42)\n",
    "        coords_2d = reducer.fit_transform(embeddings)\n",
    "    \n",
    "    # Crear DataFrame para visualización\n",
    "    viz_df = pd.DataFrame({\n",
    "        'x': coords_2d[:, 0],\n",
    "        'y': coords_2d[:, 1],\n",
    "        'product_id': distance_matrix.index,\n",
    "        'cluster': clustering_results['cluster']\n",
    "    })\n",
    "    \n",
    "    # Plotear con mejor diseño\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot 1: Clusters con colores\n",
    "    unique_clusters = sorted(viz_df['cluster'].unique())\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(unique_clusters)))\n",
    "    \n",
    "    for i, cluster in enumerate(unique_clusters):\n",
    "        cluster_data = viz_df[viz_df['cluster'] == cluster]\n",
    "        label = f'Ruido ({len(cluster_data)})' if cluster == -1 else f'Cluster {cluster} ({len(cluster_data)})'\n",
    "        \n",
    "        ax1.scatter(cluster_data['x'], cluster_data['y'], \n",
    "                   c=[colors[i]], label=label, alpha=0.7, s=50)\n",
    "    \n",
    "    ax1.set_xlabel(f'{method} Dimensión 1')\n",
    "    ax1.set_ylabel(f'{method} Dimensión 2')\n",
    "    ax1.set_title(f'Clusters visualizados con {method}')\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Densidad de puntos\n",
    "    ax2.scatter(viz_df['x'], viz_df['y'], c=viz_df['cluster'], \n",
    "               cmap='viridis', alpha=0.6, s=30)\n",
    "    ax2.set_xlabel(f'{method} Dimensión 1')\n",
    "    ax2.set_ylabel(f'{method} Dimensión 2')\n",
    "    ax2.set_title(f'Densidad de clusters - {method}')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return viz_df\n",
    "\n",
    "def create_interactive_cluster_summary(clustering_results, cluster_characteristics):\n",
    "    \"\"\"\n",
    "    Crea un resumen interactivo de los clusters usando matplotlib\n",
    "    \"\"\"\n",
    "    print(\"=== RESUMEN DETALLADO DE CLUSTERS ===\")\n",
    "    \n",
    "    # Crear figura con múltiples subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Distribución de tamaños de clusters\n",
    "    cluster_sizes = clustering_results['cluster'].value_counts().sort_index()\n",
    "    axes[0, 0].bar(range(len(cluster_sizes)), cluster_sizes.values, \n",
    "                   color='skyblue', alpha=0.7)\n",
    "    axes[0, 0].set_xlabel('Cluster ID')\n",
    "    axes[0, 0].set_ylabel('Número de productos')\n",
    "    axes[0, 0].set_title('Distribución de tamaños de clusters')\n",
    "    axes[0, 0].set_xticks(range(len(cluster_sizes)))\n",
    "    axes[0, 0].set_xticklabels(cluster_sizes.index)\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Distancias internas promedio\n",
    "    if cluster_characteristics:\n",
    "        cluster_ids = list(cluster_characteristics.keys())\n",
    "        internal_distances = [cluster_characteristics[c]['internal_avg_distance'] \n",
    "                            for c in cluster_ids]\n",
    "        \n",
    "        axes[0, 1].bar(range(len(cluster_ids)), internal_distances, \n",
    "                      color='lightcoral', alpha=0.7)\n",
    "        axes[0, 1].set_xlabel('Cluster ID')\n",
    "        axes[0, 1].set_ylabel('Distancia DTW promedio interna')\n",
    "        axes[0, 1].set_title('Cohesión interna de clusters')\n",
    "        axes[0, 1].set_xticks(range(len(cluster_ids)))\n",
    "        axes[0, 1].set_xticklabels(cluster_ids)\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Valores promedio por cluster\n",
    "        mean_values = [cluster_characteristics[c]['mean_value'] for c in cluster_ids]\n",
    "        axes[1, 0].bar(range(len(cluster_ids)), mean_values, \n",
    "                      color='lightgreen', alpha=0.7)\n",
    "        axes[1, 0].set_xlabel('Cluster ID')\n",
    "        axes[1, 0].set_ylabel('Valor promedio')\n",
    "        axes[1, 0].set_title('Valor promedio por cluster')\n",
    "        axes[1, 0].set_xticks(range(len(cluster_ids)))\n",
    "        axes[1, 0].set_xticklabels(cluster_ids)\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Correlación con tendencia temporal\n",
    "        trend_corrs = [cluster_characteristics[c]['trend_correlation'] for c in cluster_ids]\n",
    "        colors = ['red' if x < 0 else 'green' for x in trend_corrs]\n",
    "        axes[1, 1].bar(range(len(cluster_ids)), trend_corrs, \n",
    "                      color=colors, alpha=0.7)\n",
    "        axes[1, 1].set_xlabel('Cluster ID')\n",
    "        axes[1, 1].set_ylabel('Correlación con tendencia')\n",
    "        axes[1, 1].set_title('Tendencia temporal por cluster')\n",
    "        axes[1, 1].set_xticks(range(len(cluster_ids)))\n",
    "        axes[1, 1].set_xticklabels(cluster_ids)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Imprimir resumen textual\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESUMEN EJECUTIVO DE CLUSTERS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for cluster_id in sorted(clustering_results['cluster'].unique()):\n",
    "        if cluster_id == -1:\n",
    "            continue\n",
    "            \n",
    "        count = len(clustering_results[clustering_results['cluster'] == cluster_id])\n",
    "        print(f\"\\nCLUSTER {cluster_id}:\")\n",
    "        print(f\"  • Productos: {count}\")\n",
    "        \n",
    "        if cluster_characteristics and cluster_id in cluster_characteristics:\n",
    "            char = cluster_characteristics[cluster_id]\n",
    "            print(f\"  • Valor promedio: {char['mean_value']:.2f}\")\n",
    "            print(f\"  • Cohesión interna: {char['internal_avg_distance']:.2f}\")\n",
    "            print(f\"  • Tendencia: {'Creciente' if char['trend_correlation'] > 0.1 else 'Decreciente' if char['trend_correlation'] < -0.1 else 'Estable'}\")\n",
    "            print(f\"  • Variabilidad: {'Alta' if char['mean_std'] > char['mean_value'] * 0.5 else 'Media' if char['mean_std'] > char['mean_value'] * 0.2 else 'Baja'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "def analyze_cluster_characteristics(distance_matrix, clustering_results, series_matrix):\n",
    "    \"\"\"\n",
    "    Analiza las características de cada cluster\n",
    "    \"\"\"\n",
    "    print(\"=== ANÁLISIS DE CARACTERÍSTICAS DE CLUSTERS ===\")\n",
    "    \n",
    "    # Debug: verificar compatibilidad de índices\n",
    "    print(\"DEBUG: Verificando compatibilidad de índices...\")\n",
    "    print(f\"Índices en distance_matrix: {list(distance_matrix.index[:5])} ... (total: {len(distance_matrix.index)})\")\n",
    "    print(f\"Índices en series_matrix: {list(series_matrix.index[:5])} ... (total: {len(series_matrix.index)})\")\n",
    "    print(f\"Product_ids en clustering_results: {list(clustering_results['product_id'][:5])} ... (total: {len(clustering_results)})\")\n",
    "    \n",
    "    # Verificar tipos de datos\n",
    "    print(f\"Tipo de índices distance_matrix: {type(distance_matrix.index[0])}\")\n",
    "    print(f\"Tipo de índices series_matrix: {type(series_matrix.index[0])}\")\n",
    "    print(f\"Tipo de product_id en clustering: {type(clustering_results['product_id'].iloc[0])}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for cluster_id in sorted(clustering_results['cluster'].unique()):\n",
    "        if cluster_id == -1:  # Saltar ruido en DBSCAN\n",
    "            continue\n",
    "            \n",
    "        cluster_products = clustering_results[clustering_results['cluster'] == cluster_id]['product_id']\n",
    "        \n",
    "        print(f\"\\n--- CLUSTER {cluster_id} ---\")\n",
    "        print(f\"Productos: {len(cluster_products)}\")\n",
    "        \n",
    "        # Convertir a mismo tipo de dato si es necesario\n",
    "        cluster_products_list = cluster_products.tolist()\n",
    "        \n",
    "        # Verificar qué productos están disponibles en las matrices\n",
    "        available_in_distance = [p for p in cluster_products_list if p in distance_matrix.index]\n",
    "        available_in_series = [p for p in cluster_products_list if p in series_matrix.index]\n",
    "        \n",
    "        print(f\"Productos disponibles en distance_matrix: {len(available_in_distance)}/{len(cluster_products_list)}\")\n",
    "        print(f\"Productos disponibles en series_matrix: {len(available_in_series)}/{len(cluster_products_list)}\")\n",
    "        \n",
    "        if len(available_in_series) == 0:\n",
    "            print(f\"ADVERTENCIA: No se encontraron productos del cluster {cluster_id} en series_matrix\")\n",
    "            continue\n",
    "            \n",
    "        # Estadísticas de las series temporales\n",
    "        try:\n",
    "            cluster_series = series_matrix.loc[available_in_series]\n",
    "            \n",
    "            # Estadísticas descriptivas\n",
    "            mean_series = cluster_series.mean()\n",
    "            std_series = cluster_series.std()\n",
    "            \n",
    "            print(f\"Valor promedio: {mean_series.mean():.2f}\")\n",
    "            print(f\"Desviación estándar promedio: {std_series.mean():.2f}\")\n",
    "            \n",
    "            # Verificar que mean_series no esté vacío\n",
    "            if len(mean_series) > 1:\n",
    "                trend_corr = np.corrcoef(range(len(mean_series)), mean_series)[0,1]\n",
    "                print(f\"Tendencia (correlación con tiempo): {trend_corr:.3f}\")\n",
    "            else:\n",
    "                trend_corr = 0.0\n",
    "                print(\"Tendencia: No se puede calcular (datos insuficientes)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculando estadísticas de series: {e}\")\n",
    "            mean_series = pd.Series([0])\n",
    "            std_series = pd.Series([0])\n",
    "            trend_corr = 0.0\n",
    "        \n",
    "        # Cohesión interna (distancias DTW dentro del cluster)\n",
    "        try:\n",
    "            if len(available_in_distance) >= 2:\n",
    "                cluster_distances = distance_matrix.loc[available_in_distance, available_in_distance]\n",
    "                internal_distances = cluster_distances.values[np.triu_indices_from(cluster_distances.values, k=1)]\n",
    "                \n",
    "                if len(internal_distances) > 0:\n",
    "                    avg_distance = np.mean(internal_distances)\n",
    "                    max_distance = np.max(internal_distances)\n",
    "                    print(f\"Distancia DTW interna promedio: {avg_distance:.2f}\")\n",
    "                    print(f\"Distancia DTW interna máxima: {max_distance:.2f}\")\n",
    "                else:\n",
    "                    avg_distance = 0.0\n",
    "                    max_distance = 0.0\n",
    "                    print(\"Distancias DTW: No se pueden calcular (cluster muy pequeño)\")\n",
    "            else:\n",
    "                avg_distance = 0.0\n",
    "                max_distance = 0.0\n",
    "                print(\"Distancias DTW: No se pueden calcular (productos insuficientes en matriz)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculando distancias DTW: {e}\")\n",
    "            avg_distance = 0.0\n",
    "            max_distance = 0.0\n",
    "        \n",
    "        # Guardar resultados\n",
    "        results[cluster_id] = {\n",
    "            'products': cluster_products_list,\n",
    "            'products_in_distance_matrix': available_in_distance,\n",
    "            'products_in_series_matrix': available_in_series,\n",
    "            'size': len(cluster_products_list),\n",
    "            'mean_value': mean_series.mean() if not mean_series.empty else 0.0,\n",
    "            'mean_std': std_series.mean() if not std_series.empty else 0.0,\n",
    "            'trend_correlation': trend_corr,\n",
    "            'internal_avg_distance': avg_distance,\n",
    "            'internal_max_distance': max_distance,\n",
    "            'mean_series': mean_series.tolist() if not mean_series.empty else [0]\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_cluster_profiles(cluster_characteristics, series_matrix):\n",
    "    \"\"\"\n",
    "    Plotea los perfiles promedio de cada cluster\n",
    "    \"\"\"\n",
    "    print(\"=== PERFILES DE CLUSTERS ===\")\n",
    "    \n",
    "    n_clusters = len(cluster_characteristics)\n",
    "    cols = 3\n",
    "    rows = (n_clusters + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))\n",
    "    if rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, (cluster_id, data) in enumerate(cluster_characteristics.items()):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        \n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        # Obtener series del cluster\n",
    "        cluster_products = data['products']\n",
    "        cluster_series = series_matrix.loc[cluster_products]\n",
    "        \n",
    "        # Plotear series individuales (transparentes)\n",
    "        for product in cluster_products[:20]:  # Limitar para visualización\n",
    "            ax.plot(cluster_series.loc[product], alpha=0.3, color='lightblue', linewidth=0.5)\n",
    "        \n",
    "        # Plotear promedio (destacado)\n",
    "        mean_series = pd.Series(data['mean_series'], index=series_matrix.columns)\n",
    "        ax.plot(mean_series, color='red', linewidth=3, label='Promedio')\n",
    "        \n",
    "        ax.set_title(f'Cluster {cluster_id} ({data[\"size\"]} productos)')\n",
    "        ax.set_xlabel('Período')\n",
    "        ax.set_ylabel('Valor')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Ocultar subplots vacíos\n",
    "    for i in range(n_clusters, rows * cols):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        axes[row, col].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def save_clustering_results(clustering_results, cluster_characteristics, filename_prefix='clustering_results'):\n",
    "    \"\"\"\n",
    "    Guarda los resultados de clustering\n",
    "    \"\"\"\n",
    "    # Guardar asignaciones de clusters\n",
    "    clustering_results.to_csv(f'{filename_prefix}_assignments.csv', index=False)\n",
    "    \n",
    "    # Guardar características de clusters\n",
    "    import json\n",
    "    with open(f'{filename_prefix}_characteristics.json', 'w') as f:\n",
    "        json.dump(cluster_characteristics, f, indent=2)\n",
    "    \n",
    "    print(f\"Resultados guardados:\")\n",
    "    print(f\"  - {filename_prefix}_assignments.csv\")\n",
    "    print(f\"  - {filename_prefix}_characteristics.json\")\n",
    "\n",
    "# PIPELINE COMPLETO DE CLUSTERING\n",
    "def full_clustering_pipeline(distance_matrix_path='dtw_matrix_full.csv', \n",
    "                           series_matrix_path=None):\n",
    "    \"\"\"\n",
    "    Pipeline completo de clustering con DTW\n",
    "    \"\"\"\n",
    "    print(\"=== PIPELINE COMPLETO DE CLUSTERING DTW ===\")\n",
    "    \n",
    "    # 1. Cargar matriz de distancias\n",
    "    distance_matrix = load_dtw_results(distance_matrix_path)\n",
    "    \n",
    "    # 2. Cargar matriz de series (si está disponible)\n",
    "    if series_matrix_path:\n",
    "        series_matrix = pd.read_csv(series_matrix_path, index_col=0)\n",
    "    else:\n",
    "        # Recrear desde datos originales\n",
    "        import os\n",
    "        pathdata = 'data/'\n",
    "        filename = 'sell-in.txt'\n",
    "        filepath = os.path.join(pathdata, filename)\n",
    "        sell = pd.read_csv(filepath, sep=\"\\t\")\n",
    "        \n",
    "        df_agg = sell.groupby(['periodo', 'product_id'], as_index=False).agg({\n",
    "            'tn': 'sum'\n",
    "        })\n",
    "        \n",
    "        series_matrix = df_agg.pivot(index='product_id', columns='periodo', values='tn')\n",
    "        series_matrix = series_matrix.fillna(method='ffill').fillna(method='bfill').fillna(0)\n",
    "    \n",
    "    # 3. Encontrar número óptimo de clusters\n",
    "    cluster_metrics = find_optimal_clusters(distance_matrix, max_clusters=15)\n",
    "    plot_cluster_metrics(cluster_metrics)\n",
    "    \n",
    "    # 4. Aplicar clustering jerárquico\n",
    "    hierarchical_results, linkage_matrix = perform_hierarchical_clustering(distance_matrix, n_clusters=6)\n",
    "    \n",
    "    # 5. Aplicar DBSCAN\n",
    "    dbscan_results = perform_dbscan_clustering(distance_matrix, eps=None, min_samples=5)\n",
    "    \n",
    "    # 5.5. Sincronizar datos antes del análisis\n",
    "    print(\"\\n=== SINCRONIZACIÓN DE DATOS ===\")\n",
    "    hierarchical_results_sync = verify_and_sync_data(distance_matrix, hierarchical_results, series_matrix)\n",
    "    dbscan_results_sync = verify_and_sync_data(distance_matrix, dbscan_results, series_matrix)\n",
    "    \n",
    "    # 6. Visualizar resultados\n",
    "    print(\"\\n=== VISUALIZACIÓN CLUSTERING JERÁRQUICO ===\")\n",
    "    viz_hierarchical = visualize_clusters_2d(distance_matrix, hierarchical_results_sync, method='MDS')\n",
    "    \n",
    "    print(\"\\n=== VISUALIZACIÓN CLUSTERING DBSCAN ===\")\n",
    "    viz_dbscan = visualize_clusters_2d(distance_matrix, dbscan_results_sync, method='MDS')\n",
    "    \n",
    "    # 7. Analizar características (clustering jerárquico)\n",
    "    cluster_chars = analyze_cluster_characteristics(distance_matrix, hierarchical_results_sync, series_matrix)\n",
    "    \n",
    "    # 8. Crear resumen interactivo\n",
    "    create_interactive_cluster_summary(hierarchical_results, cluster_chars)\n",
    "    \n",
    "    # 9. Plotear perfiles\n",
    "    plot_cluster_profiles(cluster_chars, series_matrix)\n",
    "    \n",
    "    # 9. Guardar resultados\n",
    "    save_clustering_results(hierarchical_results, cluster_chars, 'hierarchical_clustering')\n",
    "    save_clustering_results(dbscan_results, {}, 'dbscan_clustering')\n",
    "    \n",
    "    print(\"\\n=== PIPELINE COMPLETO ===\")\n",
    "    return {\n",
    "        'distance_matrix': distance_matrix,\n",
    "        'series_matrix': series_matrix,\n",
    "        'hierarchical_results': hierarchical_results_sync,\n",
    "        'dbscan_results': dbscan_results_sync,\n",
    "        'cluster_characteristics': cluster_chars,\n",
    "        'linkage_matrix': linkage_matrix\n",
    "    }\n",
    "\n",
    "# EJEMPLO DE USO:\n",
    "\"\"\"\n",
    "# Ejecutar pipeline completo\n",
    "results = full_clustering_pipeline('dtw_matrix_full.csv')\n",
    "\n",
    "# O ejecutar pasos individuales:\n",
    "# distance_matrix = load_dtw_results('dtw_matrix_full.csv')\n",
    "# cluster_metrics = find_optimal_clusters(distance_matrix)\n",
    "# hierarchical_results, linkage_matrix = perform_hierarchical_clustering(distance_matrix, n_clusters=8)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_matrix = load_dtw_results('clusters/dtw_matrix_octubre19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56fe4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_results, linkage_matrix = perform_hierarchical_clustering(distance_matrix, n_clusters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd958fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "hierarchical_results.to_csv(\"clusters/hierarchical_results_agosto_50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8005a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = full_clustering_pipeline('dtw_matrix_full.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
